{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "import csv\n",
    "import argparse\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicGenreDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, target_transform=None, num_splits=10):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.classes, self.class_to_idx = self._find_classes(self.root)\n",
    "        self.samples = self._make_dataset(self.root, self.class_to_idx)\n",
    "        self.num_splits = num_splits\n",
    "\n",
    "        # Initialize LabelBinarizer for one-hot encoding\n",
    "        self.label_binarizer = LabelBinarizer()\n",
    "        self.label_binarizer.fit(range(len(self.classes)))\n",
    "\n",
    "    def _find_classes(self, dir):\n",
    "        classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
    "        classes.sort()\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    def _make_dataset(self, dir, class_to_idx):\n",
    "        dataset = []\n",
    "        for target in sorted(class_to_idx.keys()):\n",
    "            d = os.path.join(dir, target)\n",
    "            if not os.path.isdir(d):\n",
    "                continue\n",
    "\n",
    "            for root, _, fnames in sorted(os.walk(d)):\n",
    "                for fname in sorted(fnames):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    item = (path, class_to_idx[target])\n",
    "                    dataset.append(item)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "\n",
    "        img = np.load(path)\n",
    "\n",
    "        # Use one-hot encoding for the target\n",
    "        target_one_hot = self.label_binarizer.transform([target])[0]\n",
    "\n",
    "        # Ensure that target_one_hot has the correct length (num_classes)\n",
    "        if len(target_one_hot) != 10:\n",
    "            raise ValueError(f\"The length of target_one_hot ({len(target_one_hot)}) does not match num_classes ({10}).\")\n",
    "\n",
    "        # Cast the target label to Long data type\n",
    "        target_one_hot = torch.tensor(target_one_hot, dtype=torch.float32)  # or torch.int64\n",
    "\n",
    "        return img, target_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class CNN-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, num_classes, rnn_hidden_size, num_rnn_layers) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 128, 5, padding=2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 5, padding=2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Size of the flattened CNN features, needs to be calculated based on the CNN output\n",
    "        self.cnn_output_size = 32 * 160 * 128\n",
    "        # self.cnn_output_size = 1187840\n",
    "\n",
    "        # RNN layers\n",
    "        self.rnn = nn.GRU(input_size=self.cnn_output_size,\n",
    "                          hidden_size=rnn_hidden_size,\n",
    "                          num_layers=num_rnn_layers,\n",
    "                          batch_first=True)\n",
    "\n",
    "        # Linear and LogSoftmax layers\n",
    "        self.fc = nn.Linear(rnn_hidden_size, num_classes)\n",
    "        # self.log_softmax = nn.LogSoftmax(dim=-1) # LogSoftmax does not work with CrossEntropyLoss !!!!\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv_layers(x)\n",
    "\n",
    "        # Flatten the CNN output for the RNN layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # The RNN layer expects inputs of shape (batch, seq_len, features)\n",
    "        # Since we don't have a sequence, we can treat the entire CNN output as one sequence\n",
    "        # by adding an additional dimension with seq_len=1\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # Apply the RNN layer\n",
    "        x, _ = self.rnn(x)\n",
    "\n",
    "        # Take the output for the last time-step\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # Apply the final fully connected layer and LogSoftmax\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNtoRNN(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(1, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (rnn): GRU(655360, 1, num_layers=96, batch_first=True)\n",
      "  (fc): Linear(in_features=1, out_features=10, bias=True)\n",
      ")\n",
      "Trainging on:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Training:   0%|          | 0/27 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 129])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Training:   0%|          | 0/27 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 1, 5, 5], expected input[1, 30, 128, 129] to have 1 channels, but got 30 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\macca\\Local_Files\\GitHub\\DSAP_BIG_PROJECT\\main.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/macca/Local_Files/GitHub/DSAP_BIG_PROJECT/main.ipynb#W6sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m \u001b[39m# HERE = inputs[0]\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/macca/Local_Files/GitHub/DSAP_BIG_PROJECT/main.ipynb#W6sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/macca/Local_Files/GitHub/DSAP_BIG_PROJECT/main.ipynb#W6sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/macca/Local_Files/GitHub/DSAP_BIG_PROJECT/main.ipynb#W6sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m \u001b[39m# print(f\"Output shape:{outputs.size()}\")\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/macca/Local_Files/GitHub/DSAP_BIG_PROJECT/main.ipynb#W6sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[1;32mc:\\Users\\macca\\Local_Files\\GitHub\\DSAP_BIG_PROJECT\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\macca\\Local_Files\\GitHub\\DSAP_BIG_PROJECT\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\macca\\Local_Files\\GitHub\\DSAP_BIG_PROJECT\\main.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/macca/Local_Files/GitHub/DSAP_BIG_PROJECT/main.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/macca/Local_Files/GitHub/DSAP_BIG_PROJECT/main.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_layers(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/macca/Local_Files/GitHub/DSAP_BIG_PROJECT/main.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m# Flatten the CNN output for the RNN layer\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/macca/Local_Files/GitHub/DSAP_BIG_PROJECT/main.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\macca\\Local_Files\\GitHub\\DSAP_BIG_PROJECT\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\macca\\Local_Files\\GitHub\\DSAP_BIG_PROJECT\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\macca\\Local_Files\\GitHub\\DSAP_BIG_PROJECT\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\macca\\Local_Files\\GitHub\\DSAP_BIG_PROJECT\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\macca\\Local_Files\\GitHub\\DSAP_BIG_PROJECT\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\macca\\Local_Files\\GitHub\\DSAP_BIG_PROJECT\\.venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\macca\\Local_Files\\GitHub\\DSAP_BIG_PROJECT\\.venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 1, 5, 5], expected input[1, 30, 128, 129] to have 1 channels, but got 30 channels instead"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "num_classes = 10  # Assume there are 10 music genre classes\n",
    "batch_size = 30\n",
    "root = \"data/gtzan/data_10_split\"  # Replace with the correct path\n",
    "# Parameters\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 25\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "# Chose the model to use\n",
    "model_chosen = \"custom_2\"\n",
    "# Choose if doing Train and Test or ONLY TEST\n",
    "num_rnn_layers = 1\n",
    "rnn_hidden_size = 96\n",
    "train = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_info = {\n",
    "    'parameter': 'value',\n",
    "    'lr': f'{learning_rate:.8f}',\n",
    "    'number of epochs': num_epochs,\n",
    "    'base_model': model_chosen,\n",
    "    'device': str(device),\n",
    "    'rnn_hidden_size': 96,\n",
    "    'num_rnn_layers': 1\n",
    "}\n",
    "\n",
    "trained = False\n",
    "\n",
    "# Transformations for spectrogram images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "    \n",
    "dataset = MusicGenreDataset(root, transform=transform)\n",
    "dataset2 = MusicGenreDataset(root, transform=transform)\n",
    "\n",
    "\n",
    "show_first = False\n",
    "if show_first:\n",
    "\n",
    "    # Create a DataLoader\n",
    "    dataloader = torch.utils.data.DataLoader(dataset2, batch_size=25, shuffle=False)\n",
    "\n",
    "    # Iterate over the DataLoader and visualize the first 10 images\n",
    "    for batch in dataloader:\n",
    "        images, labels = batch\n",
    "\n",
    "        for img in images:\n",
    "            print(img.size())\n",
    "\n",
    "        break  # Break after the first batch to show only the first 10 images\n",
    "\n",
    "# Split between training, validation, and testing sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "if model_chosen == \"alexnet\":\n",
    "    # Initialize the model AlexNet with dropout\n",
    "    model = models.alexnet(pretrained=True)\n",
    "    # print(model)\n",
    "    model.classifier[6] = nn.Sequential(\n",
    "        nn.Dropout(0.2),  # Add dropout with a specified probability\n",
    "        nn.Linear(1000, num_classes),\n",
    "    )\n",
    "elif model_chosen == \"googlenet\":\n",
    "    # Initialize the model GoogLeNet with dropout\n",
    "    model = models.googlenet(pretrained=True)\n",
    "    print(model)\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.2),  # Add dropout with a specified probability\n",
    "        nn.Linear(1024, num_classes),\n",
    "    )\n",
    "elif model_chosen == \"custom_1\":\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=128, kernel_size=5),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),  # MaxPooling dopo il primo layer di convoluzione\n",
    "\n",
    "        nn.Conv2d(in_channels=128, out_channels=128, kernel_size=5),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),  # MaxPooling dopo il secondo layer di convoluzione\n",
    "\n",
    "        nn.Flatten(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(157 * 29 * 128, 10),\n",
    "        nn.LogSoftmax(dim=-1)\n",
    "    )\n",
    "elif model_chosen == \"custom_2\":\n",
    "    \n",
    "    model = CNNtoRNN(num_classes, num_rnn_layers, rnn_hidden_size)\n",
    "    \n",
    "    \n",
    "print(model)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training and validation loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "if (train):\n",
    "\n",
    "    trained = True\n",
    "\n",
    "    print(\"Trainging on: \", device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} - Training\"):\n",
    "            # print(f\"Input shape:{inputs.size()}\")\n",
    "            # print(f\"Labels shape:{labels}\")\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # inputs = inputs.unsqueeze(1)  # Adds a channel dimension\n",
    "\n",
    "            print(inputs[0].size())\n",
    "\n",
    "            # HERE = inputs[0]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            # print(f\"Output shape:{outputs.size()}\")\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # Update this line to use one-hot encoded labels\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "        average_train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_losses.append(average_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        model_info[f\"train loss epoch \" + str(epoch)] = f'{average_train_loss:.4f}'\n",
    "        model_info[f\"train accuracy epoch \" + str(epoch)] = f'{train_accuracy:.4f}'\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs_val, labels_val in tqdm(val_loader, desc=\"Validation\"):\n",
    "                inputs_val, labels_val = inputs_val.to(device), labels_val.to(device)\n",
    "                # inputs_val = inputs_val.unsqueeze(1)  # Adds a channel dimension\n",
    "\n",
    "                outputs_val = model(inputs_val)\n",
    "                val_loss = criterion(outputs_val, labels_val)\n",
    "                running_val_loss += val_loss.item()\n",
    "\n",
    "                _, predicted_val = torch.max(outputs_val.data, 1)\n",
    "                total_val += labels_val.size(0)\n",
    "                correct_val += (predicted_val == torch.argmax(labels_val, dim=1)).sum().item()\n",
    "\n",
    "\n",
    "        average_val_loss = running_val_loss / len(val_loader)\n",
    "        val_accuracy = correct_val / total_val\n",
    "        val_losses.append(average_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        model_info[f\"val loss epoch \" + str(epoch)] = f'{average_val_loss:.4f}'\n",
    "        model_info[f\"val accuracy epoch \" + str(epoch)] = f'{val_accuracy:.4f}'\n",
    "\n",
    "        # save the model\n",
    "        newpath = \"models/\" + model_chosen + \"_\" + timestr\n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "\n",
    "        model_name = str(timestr) + \"_\" + str(epoch) + \".pth\"\n",
    "        # Save the trained model\n",
    "        torch.save(model.state_dict(), 'models/' + model_chosen + \"_\" + timestr + '/' + model_name)\n",
    "\n",
    "        # Print training and validation metrics for the epoch\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                f\"Train Loss: {average_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "                f\"Val Loss: {average_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    model_name = timestr + \".pth\"\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'models/last.pth')\n",
    "\n",
    "    # Plotting the training and validation losses\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "    plt.plot(val_losses, label='Validation Loss', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # # Annotate each point with its exact value\n",
    "    # for i, value in enumerate(train_losses):\n",
    "    #     plt.text(i, value, f'{value:.2f}', ha='center', va='bottom', fontsize=5)\n",
    "\n",
    "    # for i, value in enumerate(val_losses):\n",
    "    #     plt.text(i, value, f'{value:.2f}', ha='center', va='bottom',fontsize=5)\n",
    "\n",
    "    # Plotting the training and validation accuracies\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Training Accuracy', color='blue')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # # Annotate each point with its exact value\n",
    "    # for i, value in enumerate(train_accuracies):\n",
    "    #     plt.text(i, value, f'{value:.2f}', ha='center', va='bottom')\n",
    "\n",
    "    # for i, value in enumerate(val_accuracies):\n",
    "    #     plt.text(i, value, f'{value:.2f}', ha='center', va='bottom')\n",
    "\n",
    "    save_fig_name = timestr + \".png\"\n",
    "    plt.savefig(\"save/\" + model_chosen + \"_\" + save_fig_name)\n",
    "    plt.savefig(\"models/\" + model_chosen + \"_\" + timestr + \"/\" + save_fig_name)\n",
    "    plt.show()\n",
    "\n",
    "# Test the model\n",
    "# Load the saved model state\n",
    "model.load_state_dict(torch.load('models/last.pth'))\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Testing on: \", device)\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predicted = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # inputs = inputs.unsqueeze(1)  # Adds a channel dimension\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        # Update this line to use one-hot encoded labels\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "model_info[\"accuracy\"] = f'{accuracy:.4f}'\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_predicted)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
    "\n",
    "# Add confusion matrix and F1 score to the model_info dictionary\n",
    "model_info['confusion_matrix'] = conf_matrix.tolist()  # Convert to list for JSON serialization\n",
    "model_info['f1_score'] = f1\n",
    "\n",
    "if trained:\n",
    "    # Save the model information in a CSV file\n",
    "    csv_file_path = \"models/\" + model_chosen + \"_\" + timestr + \"/\" + \"results.csv\"\n",
    "else:\n",
    "    csv_file_path = \"last.csv\"\n",
    "\n",
    "with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    for key, value in model_info.items():\n",
    "        # if isinstance(value, list):\n",
    "        #     csv_writer.writerow([key] + value)\n",
    "        # else:\n",
    "        csv_writer.writerow([key, value])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
